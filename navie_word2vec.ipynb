{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "flatten = lambda l:[item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    start = 0\n",
    "    end = batch_size\n",
    "    length = len(train_data)\n",
    "    while end < length:\n",
    "        batch = train_data[start:end]\n",
    "        tmp = end\n",
    "        end = end + batch_size\n",
    "        start = tmp\n",
    "        yield batch\n",
    "    if end >= length:\n",
    "        batch = train_data[start:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda x: word2index[x] if word2index.get(x) is not None else word2index['<UNK>'],seq))\n",
    "    return idxs\n",
    "def prepare_word(word, word2index):\n",
    "    return word2index[word] if word2index.get(word) is not None else word2index['<UNK>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load and Preprocessing\n",
    "\n",
    "### Load corpus : gutenberg corpus\n",
    "[NLTK load data](https://www.nltk.org/data.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg as bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bg.sents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = list(nltk.corpus.gutenberg.sents('melville-moby_dick.txt'))[:100] # sampling sentences for test\n",
    "\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']'],\n",
       " ['etymology', '.'],\n",
       " ['(',\n",
       "  'supplied',\n",
       "  'by',\n",
       "  'a',\n",
       "  'late',\n",
       "  'consumptive',\n",
       "  'usher',\n",
       "  'to',\n",
       "  'a',\n",
       "  'grammar',\n",
       "  'school',\n",
       "  ')'],\n",
       " ['the',\n",
       "  'pale',\n",
       "  'usher',\n",
       "  '--',\n",
       "  'threadbare',\n",
       "  'in',\n",
       "  'coat',\n",
       "  ',',\n",
       "  'heart',\n",
       "  ',',\n",
       "  'body',\n",
       "  ',',\n",
       "  'and',\n",
       "  'brain',\n",
       "  ';',\n",
       "  'i',\n",
       "  'see',\n",
       "  'him',\n",
       "  'now',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'was',\n",
       "  'ever',\n",
       "  'dusting',\n",
       "  'his',\n",
       "  'old',\n",
       "  'lexicons',\n",
       "  'and',\n",
       "  'grammars',\n",
       "  ',',\n",
       "  'with',\n",
       "  'a',\n",
       "  'queer',\n",
       "  'handkerchief',\n",
       "  ',',\n",
       "  'mockingly',\n",
       "  'embellished',\n",
       "  'with',\n",
       "  'all',\n",
       "  'the',\n",
       "  'gay',\n",
       "  'flags',\n",
       "  'of',\n",
       "  'all',\n",
       "  'the',\n",
       "  'known',\n",
       "  'nations',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'loved',\n",
       "  'to',\n",
       "  'dust',\n",
       "  'his',\n",
       "  'old',\n",
       "  'grammars',\n",
       "  ';',\n",
       "  'it',\n",
       "  'somehow',\n",
       "  'mildly',\n",
       "  'reminded',\n",
       "  'him',\n",
       "  'of',\n",
       "  'his',\n",
       "  'mortality',\n",
       "  '.'],\n",
       " ['\"',\n",
       "  'while',\n",
       "  'you',\n",
       "  'take',\n",
       "  'in',\n",
       "  'hand',\n",
       "  'to',\n",
       "  'school',\n",
       "  'others',\n",
       "  ',',\n",
       "  'and',\n",
       "  'to',\n",
       "  'teach',\n",
       "  'them',\n",
       "  'by',\n",
       "  'what',\n",
       "  'name',\n",
       "  'a',\n",
       "  'whale',\n",
       "  '-',\n",
       "  'fish',\n",
       "  'is',\n",
       "  'to',\n",
       "  'be',\n",
       "  'called',\n",
       "  'in',\n",
       "  'our',\n",
       "  'tongue',\n",
       "  'leaving',\n",
       "  'out',\n",
       "  ',',\n",
       "  'through',\n",
       "  'ignorance',\n",
       "  ',',\n",
       "  'the',\n",
       "  'letter',\n",
       "  'h',\n",
       "  ',',\n",
       "  'which',\n",
       "  'almost',\n",
       "  'alone',\n",
       "  'maketh',\n",
       "  'the',\n",
       "  'signification',\n",
       "  'of',\n",
       "  'the',\n",
       "  'word',\n",
       "  ',',\n",
       "  'you',\n",
       "  'deliver',\n",
       "  'that',\n",
       "  'which',\n",
       "  'is',\n",
       "  'not',\n",
       "  'true',\n",
       "  '.\"'],\n",
       " ['--', 'hackluyt'],\n",
       " ['\"', 'whale', '.'],\n",
       " ['...', 'sw', '.', 'and', 'dan', '.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'moby',\n",
       " 'dick',\n",
       " 'by',\n",
       " 'herman',\n",
       " 'melville',\n",
       " '1851',\n",
       " ']',\n",
       " 'etymology',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten(corpus)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Extract Stopwords from ungram distribution tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count = Counter(flatten(corpus))\n",
    "border = int(len(word_count)*0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 96),\n",
       " ('.', 66),\n",
       " ('the', 58),\n",
       " ('of', 36),\n",
       " ('and', 35),\n",
       " ('--', 27),\n",
       " ('\"', 26),\n",
       " ('.\"', 26),\n",
       " ('to', 25),\n",
       " ('-', 24)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = word_count.most_common()[:border] + list(reversed(word_count.most_common()))[:border]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = [s[0] for s in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '.', 'the', 'of', 'and', 'man', 'artificial', 'civitas', '--(', 'state']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = list(set(flatten(corpus)) - set(stopwords))\n",
    "vocab.append('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592 583\n"
     ]
    }
   ],
   "source": [
    "print(len(set(flatten(corpus))), len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index = {'<UNK>' : 0}\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "\n",
    "index2word = {v:k for k,v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.ngrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 3\n",
    "windows = flatten([list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + c + ['<DUMMY>'] * WINDOW_SIZE, WINDOW_SIZE * 2 + 1))for c in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by'),\n",
       " ('<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'moby'), ('[', 'dick'), ('[', 'by'), ('moby', '['), ('moby', 'dick'), ('moby', 'by')]\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "\n",
    "\n",
    "for window in windows:\n",
    "    for i in range(WINDOW_SIZE * 2 + 1):\n",
    "        if i == WINDOW_SIZE or window[i] == '<DUMMY>':\n",
    "            continue\n",
    "        train_data.append((window[WINDOW_SIZE],window[i]))\n",
    "print(train_data[:WINDOW_SIZE * 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tr in train_data:\n",
    "    X_train.append(prepare_word(tr[0], word2index))\n",
    "    Y_train.append(prepare_word(tr[1],word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = list(zip(X_train,Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "318\n",
      "198\n",
      "394\n",
      "198\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(train_data[i][0])\n",
    "    print(train_data[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7606"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model,layers,Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SkipGram(Model):\n",
    "    \n",
    "    def __init__(self, vocab_size,projection_dim):\n",
    "        super(SkipGram,self).__init__()\n",
    "        self.I_H = layers.Embedding(vocab_size,projection_dim) # input_hidden matrix\n",
    "        self.H_U = layers.Embedding(vocab_size,projection_dim) # hidden_out matrix\n",
    "        \n",
    "    def call(self,inputs,predict,normal):\n",
    "        inputs_embed = self.I_H(inputs)\n",
    "        predict_embed = self.H_U(predict)\n",
    "        normal_embed = self.H_U(normal)\n",
    "        \n",
    "        scores = tf.matmul(predict_embed,tf.transpose(inputs_embed,[0,2,1])) # Bx1xD * BxDx1 => Bx1\n",
    "#         print(\"predict shape:{} input shape:{} result shape{}\".format(\n",
    "#                 predict_embed.shape,tf.transpose(inputs_embed,[0,2,1]).shape,scores.shape))\n",
    "        scores = tf.squeeze(scores,2)\n",
    "        norm_scores = tf.squeeze(tf.matmul(normal_embed,tf.transpose(inputs_embed,[0,2,1])),2) # BxVxD * BxDx1 => BxV\n",
    "        \n",
    "        nll = tf.expand_dims(-tf.math.reduce_mean(tf.math.log(tf.math.exp(scores)/tf.math.reduce_sum(tf.math.exp(norm_scores), 1),1)),axis=0) # log-softmax\n",
    "        \n",
    "        return nll # negative log likelihood\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 30\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 100\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, mean_loss : 6.37\n",
      "Epoch : 10, mean_loss : 5.98\n",
      "Epoch : 20, mean_loss : 5.60\n",
      "Epoch : 30, mean_loss : 5.39\n",
      "Epoch : 40, mean_loss : 5.24\n",
      "Epoch : 50, mean_loss : 5.11\n",
      "Epoch : 60, mean_loss : 4.99\n",
      "Epoch : 70, mean_loss : 4.88\n",
      "Epoch : 80, mean_loss : 4.77\n",
      "Epoch : 90, mean_loss : 4.68\n"
     ]
    }
   ],
   "source": [
    "model = SkipGram(len(word2index), EMBEDDING_SIZE)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "for epoch in range(EPOCH):\n",
    "    for i, batch in enumerate(get_batch(BATCH_SIZE, train_data)):\n",
    "        inputs, targets = zip(*batch)\n",
    "        vocabs = tf.expand_dims(tf.convert_to_tensor(prepare_sequence(list(vocab), word2index)),0)\n",
    "        inputs = tf.expand_dims(tf.convert_to_tensor(inputs),1)\n",
    "        targets = tf.expand_dims(tf.convert_to_tensor(targets),1)\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = model(inputs, targets, vocabs)\n",
    "            grads = tape.gradient(loss,model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            losses.append(loss.numpy().tolist()[0])\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch : %d, mean_loss : %.02f\" % (epoch,np.mean(losses)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:practices]",
   "language": "python",
   "name": "conda-env-practices-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
